{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.3 ~ 2.1.4 Estimator & Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf110/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf #텐서플로우 모듈 불러오기\n",
    "\n",
    "from tensorflow.keras.datasets import imdb #imdb 한글 데이터셋을 불러온다\n",
    "from tensorflow import keras #전처리를 위한 processing 기능이다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기 기본 데이터를 불러옵니다.\n",
    "\n",
    "  - Estimator의 설명을 위해, 데이터 로드 및 기본 기능 (모델 구조)는 https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "    를 참고하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape: (25000, 256)\n",
      "eval_input shape: (25000, 256)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000 #문장의 단어 사이즈\n",
    "SENT_SIZE = 256 #문장 길이\n",
    "BATCH_SIZE = 128\n",
    "EMB_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "PAD_ID = 0 # 200단어 이하 문장에 대해서 0 값을 채워 넣는다.\n",
    "START_ID = 1 # 시작 id 값\n",
    "OOV_ID = 2 # out of vocab\n",
    "INDEX_OFFSET = 2\n",
    "\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "#IMDB 데이터셋을 로드 합니다. 학습과 테스트 셋으로 나눕니다.\n",
    "(train_data, train_label), (eval_data, eval_label) = imdb.load_data(num_words=VOCAB_SIZE,\n",
    "                                                      start_char=START_ID,\n",
    "                                                      oov_char=OOV_ID,\n",
    "                                                      index_from=INDEX_OFFSET)\n",
    "\n",
    "# 각 문장의 길이를 200으로 정하고, 256개가 안되는 문장에 대해서는 패딩(0)값으로 채워줍니다.\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=SENT_SIZE)\n",
    "eval_data = keras.preprocessing.sequence.pad_sequences(eval_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=SENT_SIZE)\n",
    "\n",
    "train_label = train_label.astype('float32').reshape(-1, 1)\n",
    "eval_label = eval_label.astype('float32').reshape(-1, 1)\n",
    "\n",
    "train_len = np.array([min(len(x), SENT_SIZE) for x in train_data])\n",
    "eval_len = np.array([min(len(x), SENT_SIZE) for x in eval_data])\n",
    "\n",
    "print(\"train_input shape:\", train_data.shape)\n",
    "print(\"eval_input shape:\", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index to Sentence 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> i but that or viewer apart shocking impression only 5 embarrassment story merit <UNUSED> same an woman the not too or dance many some mostly <UNK> to by history true the watching <UNUSED> few many makes <UNK> you're mean so <UNUSED> few reads state was highly who it called <UNUSED> can't some that and i'm murdered for i but <UNUSED> judge questionable seemed <UNUSED> but can first in that or viewer who we this it dad <UNUSED> but was favorite was in that whose as <UNK> the my came in of during of characters the <UNUSED> send primitive that history story sloppy all <UNUSED> your in that who exactly the not show there an these has not bother all and but in gets his much what the i human that do <UNK> of <UNUSED> watch acting profession this woman <UNUSED> <UNK> a carpenter the entertainment an can or viewer \\x96 you doing everyone it's a <UNUSED> <UNK> spoilers it after great <UNUSED> start this watched make one genuinely very you go and take northam as <UNUSED> there's but movie here \\x96 you history the through he penny as there an his family most not after <UNUSED> there's only that who club great in that reason the that reaching being movies one this that objects for cast one <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fn(X, Y=None):\n",
    "    input, label = {'x': X}, Y\n",
    "    return input, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_data))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_data, eval_label))\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    \"\"\"\n",
    "    Model is from official website: https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "    \"\"\"\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    input_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features['x'])\n",
    "    lstm = keras.layers.LSTM(32)(input_layer)\n",
    "    dense_layer = keras.layers.Dense(16, activation=tf.nn.relu)(lstm)\n",
    "    output_layer = keras.layers.Dense(1, activation=tf.nn.sigmoid)(dense_layer)\n",
    "    \n",
    "#     print(flatten)\n",
    "#     print(dense_layer)\n",
    "#     print(output_layer)\n",
    "#     print(labels)\n",
    "\n",
    "    if PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions={\n",
    "                      'pos_neg': output_layer\n",
    "                  })\n",
    "\n",
    "    loss = tf.reduce_mean(keras.metrics.binary_crossentropy(y_true=labels, y_pred=output_layer))\n",
    "    \n",
    "    if EVAL:\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(output_layer))\n",
    "        eval_metric_ops = {'acc': accuracy}\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  eval_metric_ops= eval_metric_ops,\n",
    "                  loss=loss)\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/sinseongjin/github/DeepNLP/7.NLPBOOK/2.NLP_PREP/data_out/checkpoint/tutorial/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 100, '_session_config': None, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11f48ba20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/2.NLP_PREP/data_out/checkpoint/tutorial/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/2.NLP_PREP/data_out/checkpoint/tutorial/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7032764, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.87831\n",
      "INFO:tensorflow:loss = 0.6935006, step = 101 (34.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.08347\n",
      "INFO:tensorflow:loss = 0.6932381, step = 201 (32.428 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 297 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/2.NLP_PREP/data_out/checkpoint/tutorial/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.97558\n",
      "INFO:tensorflow:loss = 0.693205, step = 301 (33.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.0205\n",
      "INFO:tensorflow:loss = 0.6932016, step = 401 (33.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.87016\n",
      "INFO:tensorflow:loss = 0.6825902, step = 501 (34.827 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 589 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/2.NLP_PREP/data_out/checkpoint/tutorial/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.8962\n",
      "INFO:tensorflow:loss = 0.69003737, step = 601 (34.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.82125\n",
      "INFO:tensorflow:loss = 0.64559734, step = 701 (35.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.38611\n",
      "INFO:tensorflow:loss = 0.6894245, step = 801 (41.913 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 848 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/2.NLP_PREP/data_out/checkpoint/tutorial/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.48179\n",
      "INFO:tensorflow:loss = 0.6629044, step = 901 (40.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.71219\n",
      "INFO:tensorflow:loss = 0.67016315, step = 1001 (36.871 sec)\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"data_out/checkpoint/tutorial/\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "config_tf = tf.estimator.RunConfig()\n",
    "config_tf._save_checkpoints_secs = 100\n",
    "config_tf._keep_checkpoint_max =  2\n",
    "config_tf._log_step_count_steps = 100\n",
    "\n",
    "model_basic = tf.estimator.Estimator(model_fn, model_dir=model_dir, config=config_tf)\n",
    "model_basic.train(train_input_fn) #학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
